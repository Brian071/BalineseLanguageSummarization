{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization Model Comparison\n",
    "\n",
    "This notebook implements a comparative study of various machine learning models for extractive summarization. We treat the summarization task as a binary classification problem at the sentence level.\n",
    "\n",
    "**Goal:** Predict whether a sentence belongs to the summary (`Label_Final = 1`) or not (`0`) based on extracted features.\n",
    "\n",
    "**Models Compared:**\n",
    "1. Support Vector Machine (SVM) - *Primary Interest*\n",
    "2. Random Forest (RF)\n",
    "3. Logistic Regression (LR)\n",
    "4. K-Nearest Neighbors (KNN)\n",
    "5. Naive Bayes (NB)\n",
    "\n",
    "**Methodology:**\n",
    "- **Baseline:** Default hyperparameters.\n",
    "- **Optimization:** Hyperparameter tuning using Optuna.\n",
    "- **Evaluation:** Stratified K-Fold Cross-Validation (k=5).\n",
    "- **Metrics:** Accuracy, Precision, Recall, F1-Score, Cohen's Kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data_preprocessing import prepare_data\n",
    "from src.model_trainer import get_models, evaluate_model\n",
    "from src.optimizer import optimize_model\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "We load the dataset, handle missing values, and scale the features. Scaling is crucial for distance-based models like SVM and KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "X, y = prepare_data('korpus_mentah_new.csv')\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts(normalize=True))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Evaluation\n",
    "\n",
    "We evaluate all 5 models using their default hyperparameters to establish a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = get_models()\n",
    "baseline_results = []\n",
    "\n",
    "print(\"Running Baseline Evaluation...\")\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    metrics = evaluate_model(model, X, y)\n",
    "    metrics['Model'] = name\n",
    "    metrics['Type'] = 'Baseline'\n",
    "    baseline_results.append(metrics)\n",
    "\n",
    "df_baseline = pd.DataFrame(baseline_results)\n",
    "df_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Optimization (Optuna)\n",
    "\n",
    "We use Optuna to find the optimal hyperparameters for each model. We maximize the **F1-Score**, as it balances Precision and Recall, which is important for summarization tasks (we want to select relevant sentences without missing too many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_results = []\n",
    "best_params_log = {}\n",
    "\n",
    "print(\"Running Optuna Optimization (this may take a while)...\")\n",
    "for name in baseline_models.keys():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    # Run optimization (50 trials per model)\n",
    "    best_model, best_params, best_score = optimize_model(name, X, y, n_trials=50)\n",
    "    \n",
    "    best_params_log[name] = best_params\n",
    "    print(f\"  Best F1: {best_score:.4f}\")\n",
    "    \n",
    "    # Evaluate the optimized model using the same CV strategy\n",
    "    metrics = evaluate_model(best_model, X, y)\n",
    "    metrics['Model'] = name\n",
    "    metrics['Type'] = 'Optimized'\n",
    "    optimized_results.append(metrics)\n",
    "\n",
    "df_optimized = pd.DataFrame(optimized_results)\n",
    "df_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in best_params_log.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(params)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison & Analysis\n",
    "\n",
    "We combine the results to compare the performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_baseline, df_optimized], ignore_index=True)\n",
    "df_final.sort_values(by=['Model', 'Type'], inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: F1-Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=df_final, x='Model', y='F1-Score', hue='Type', palette='viridis')\n",
    "plt.title('Model Comparison: Baseline vs Optimized (F1-Score)', fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(title='Configuration', loc='lower right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Cohen's Kappa Comparison\n",
    "Cohen's Kappa is a robust metric for inter-rater agreement, useful here to see how much better the model is compared to random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=df_final, x='Model', y='Kappa', hue='Type', palette='magma')\n",
    "plt.title(\"Model Comparison: Baseline vs Optimized (Cohen's Kappa)\", fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(title='Configuration', loc='lower right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The tables and charts above summarize the performance of SVM against Random Forest, Logistic Regression, KNN, and Naive Bayes.\n",
    "\n",
    "**Key Observations:**\n",
    "1. **Impact of Optimization:** Check if Optuna significantly improved the scores. SVM and KNN often benefit most from scaling and tuning.\n",
    "2. **Best Model:** Identify which model achieved the highest F1-Score and Kappa.\n",
    "3. **Consistency:** Look at the stability of results.\n",
    "\n",
    "This analysis helps select the most robust model for the text summarization pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}